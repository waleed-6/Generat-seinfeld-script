{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# load in data\n",
    "import helper\n",
    "data_dir = './data/Seinfeld_Scripts.txt'\n",
    "text = helper.load_data(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Stats\n",
      "Roughly the number of unique words: 46367\n",
      "Number of lines: 109233\n",
      "Average number of words in each line: 5.544240293684143\n",
      "\n",
      "The lines 0 to 10:\n",
      "jerry: do you know what this is all about? do you know, why were here? to be out, this is out...and out is one of the single most enjoyable experiences of life. people...did you ever hear people talking about we should go out? this is what theyre talking about...this whole thing, were all out now, no one is home. not one person here is home, were all out! there are people trying to find us, they dont know where we are. (on an imaginary phone) did you ring?, i cant find him. where did he go? he didnt tell me where he was going. he must have gone out. you wanna go out you get ready, you pick out the clothes, right? you take the shower, you get all ready, get the cash, get your friends, the car, the spot, the reservation...then youre standing around, what do you do? you go we gotta be getting back. once youre out, you wanna get back! you wanna go to sleep, you wanna get up, you wanna go out again tomorrow, right? where ever you are in life, its my feeling, youve gotta go. \n",
      "\n",
      "jerry: (pointing at georges shirt) see, to me, that button is in the worst possible spot. the second button literally makes or breaks the shirt, look at it. its too high! its in no-mans-land. you look like you live with your mother. \n",
      "\n",
      "george: are you through? \n",
      "\n",
      "jerry: you do of course try on, when you buy? \n",
      "\n",
      "george: yes, it was purple, i liked it, i dont actually recall considering the buttons. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "view_line_range = (0, 10)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "print('Dataset Stats')\n",
    "print('Roughly the number of unique words: {}'.format(len({word: None for word in text.split()})))\n",
    "\n",
    "lines = text.split('\\n')\n",
    "print('Number of lines: {}'.format(len(lines)))\n",
    "word_count_line = [len(line.split()) for line in lines]\n",
    "print('Average number of words in each line: {}'.format(np.average(word_count_line)))\n",
    "\n",
    "print()\n",
    "print('The lines {} to {}:'.format(*view_line_range))\n",
    "print('\\n'.join(text.split('\\n')[view_line_range[0]:view_line_range[1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import problem_unittests as tests\n",
    "from collections import Counter\n",
    "\n",
    "def create_lookup_tables(text):\n",
    "    \"\"\"\n",
    "    Create lookup tables for vocabulary\n",
    "    :param text: The text of tv scripts split into words\n",
    "    :return: A tuple of dicts (vocab_to_int, int_to_vocab)\n",
    "    \"\"\"\n",
    "    # TODO: Implement Function\n",
    "    vocab = set(text)\n",
    "    vocab_to_int, int_to_vocab = {}, {}\n",
    "    for i, w in enumerate(vocab):\n",
    "        vocab_to_int[w] = i\n",
    "        int_to_vocab[i] = w\n",
    "    return (vocab_to_int, int_to_vocab)    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_create_lookup_tables(create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def token_lookup():\n",
    "     \n",
    "    token={'.':'||Period||',\n",
    "           ',':'||Comma||',\n",
    "           '\"':'||Quotationmark||',\n",
    "           ';':'||Semicolon||',\n",
    "           '!':'||Exclamationmark||',\n",
    "           '(':'||LeftParentheses||',\n",
    "           ')':'||RightParentheses||',\n",
    "           '-':'||Dash||',\n",
    "           '\\n':'||Return||',\n",
    "           '?':'||Questionmark||'}\n",
    "    return token\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_tokenize(token_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "helper.preprocess_and_save_data(data_dir, token_lookup, create_lookup_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "int_text, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "train_on_gpu = torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def batch_data(words, sequence_length, batch_size):\n",
    "    feature_tensors=[]\n",
    "    target_tensors=[]\n",
    "    for i in range(0,len(words)-1):\n",
    "        if i+sequence_length+1>len(words)-1:\n",
    "            break\n",
    "        feature_tensors.append(words[i:i+sequence_length])\n",
    "        target_tensors.append(words[i+sequence_length])\n",
    "    feature_tensors=np.array(feature_tensors)\n",
    "    target_tensors=np.array(target_tensors)\n",
    "    feature_tensors=torch.from_numpy(feature_tensors)\n",
    "    target_tensors=torch.from_numpy(target_tensors)\n",
    "    data=TensorDataset(feature_tensors,target_tensors)   \n",
    "    data_loader=DataLoader(data,batch_size=batch_size)\n",
    "        \n",
    "    \n",
    "   \n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "tensor([[  0,   1,   2,   3,   4],\n",
      "        [  1,   2,   3,   4,   5],\n",
      "        [  2,   3,   4,   5,   6],\n",
      "        [  3,   4,   5,   6,   7],\n",
      "        [  4,   5,   6,   7,   8],\n",
      "        [  5,   6,   7,   8,   9],\n",
      "        [  6,   7,   8,   9,  10],\n",
      "        [  7,   8,   9,  10,  11],\n",
      "        [  8,   9,  10,  11,  12],\n",
      "        [  9,  10,  11,  12,  13]])\n",
      "\n",
      "torch.Size([10])\n",
      "tensor([  5,   6,   7,   8,   9,  10,  11,  12,  13,  14])\n"
     ]
    }
   ],
   "source": [
    "test_text = range(50)\n",
    "t_loader = batch_data(test_text, sequence_length=5, batch_size=10)\n",
    "\n",
    "data_iter = iter(t_loader)\n",
    "sample_x, sample_y = data_iter.next()\n",
    "\n",
    "print(sample_x.shape)\n",
    "print(sample_x)\n",
    "print()\n",
    "print(sample_y.shape)\n",
    "print(sample_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Initialize the PyTorch RNN Module\n",
    "        :param vocab_size: The number of input dimensions of the neural network (the size of the vocabulary)\n",
    "        :param output_size: The number of output dimensions of the neural network\n",
    "        :param embedding_dim: The size of embeddings, should you choose to use them        \n",
    "        :param hidden_dim: The size of the hidden layer outputs\n",
    "        :param dropout: dropout to add in between LSTM/GRU layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.vocab_size=vocab_size\n",
    "        self.output_size=output_size\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "        self.dropout=dropout\n",
    "        self.embedd=nn.Embedding(self.vocab_size,self.embedding_dim)\n",
    "        self.LSTM=nn.LSTM(self.embedding_dim,self.hidden_dim,self.n_layers,dropout=self.dropout,batch_first=True)\n",
    "        self.fc1=nn.Linear(self.hidden_dim,output_size)\n",
    "        \n",
    "    \n",
    "    def forward(self, nn_input, hidden):\n",
    "        batch_size=nn_input.size(0)\n",
    "        emdedds=self.embedd(nn_input)\n",
    "        lstm_out,hidden=self.LSTM(emdedds,hidden)\n",
    "        lstm_out=lstm_out.contiguous().view(-1,self.hidden_dim)\n",
    "        output=self.fc1(lstm_out)\n",
    "        outputF= output.view(batch_size, -1, self.output_size)\n",
    "        outputF=outputF[:,-1]\n",
    "        return outputF, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "       \n",
    "        weight=next(self.parameters()).data\n",
    "        \n",
    "        if train_on_gpu:\n",
    "            hidden=(weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().cuda(),\n",
    "                   weight.new(self.n_layers,batch_size,self.hidden_dim).zero_().cuda())\n",
    "        else:\n",
    "            hidden=(weight.new(self.n_layers,batch_size,self.hidden_dim).zero_(),\n",
    "                   weight.new(self.n_layers,batch_size,self.hidden_dim).zero_())\n",
    "        # initialize hidden state with zero weights, and move to GPU if available\n",
    "        \n",
    "        return hidden\n",
    "\n",
    "\n",
    "tests.test_rnn(RNN, train_on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def forward_back_prop(rnn, optimizer, criterion, inp, target, hidden):\n",
    "   \n",
    "    hidden = tuple([each.data for each in hidden])\n",
    "    if train_on_gpu:\n",
    "        inp=inp.cuda()\n",
    "        rnn=rnn.cuda()\n",
    "        target=target.cuda()\n",
    "    output,hidden=rnn(inp,hidden)\n",
    "    loss=criterion(output,target)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    nn.utils.clip_grad_norm_(rnn.parameters(), 5)\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item(), hidden\n",
    "\n",
    "tests.test_forward_back_prop(RNN, forward_back_prop, train_on_gpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_rnn(rnn, batch_size, optimizer, criterion, n_epochs, show_every_n_batches=100):\n",
    "    batch_losses = []\n",
    "    \n",
    "    rnn.train()\n",
    "\n",
    "    print(\"Training for %d epoch(s)...\" % n_epochs)\n",
    "    for epoch_i in keep_awake(range(1, n_epochs + 1)):\n",
    "        \n",
    "       \n",
    "        hidden = rnn.init_hidden(batch_size)\n",
    "        \n",
    "        for batch_i, (inputs, labels) in enumerate(train_loader, 1):\n",
    "            \n",
    "            n_batches = len(train_loader.dataset)//batch_size\n",
    "            if(batch_i > n_batches):\n",
    "                break\n",
    "            \n",
    "            loss, hidden = forward_back_prop(rnn, optimizer, criterion, inputs, labels, hidden)          \n",
    "            batch_losses.append(loss)\n",
    "\n",
    "           \n",
    "            if batch_i % show_every_n_batches == 0:\n",
    "                print('Epoch: {:>4}/{:<4}  Loss: {}\\n'.format(\n",
    "                    epoch_i, n_epochs, np.average(batch_losses)))\n",
    "                batch_losses = []\n",
    "\n",
    "    return rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sequence_length = 15  \n",
    "batch_size = 128\n",
    "\n",
    "\n",
    "train_loader = batch_data(int_text, sequence_length, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 5\n",
    "\n",
    "learning_rate = 0.001\n",
    "\n",
    "\n",
    "vocab_size = len(vocab_to_int)\n",
    "\n",
    "output_size = len(vocab_to_int)\n",
    "\n",
    "embedding_dim = 400\n",
    "\n",
    "hidden_dim = 500\n",
    "\n",
    "n_layers = 3\n",
    "\n",
    "show_every_n_batches = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5 epoch(s)...\n",
      "Epoch:    1/5     Loss: 5.831405641078949\n",
      "\n",
      "Epoch:    1/5     Loss: 5.761553304195404\n",
      "\n",
      "Epoch:    1/5     Loss: 5.904258048534393\n",
      "\n",
      "Epoch:    1/5     Loss: 5.857336911201477\n",
      "\n",
      "Epoch:    1/5     Loss: 4.801289212465286\n",
      "\n",
      "Epoch:    1/5     Loss: 4.695549366474151\n",
      "\n",
      "Epoch:    2/5     Loss: 4.443392631061916\n",
      "\n",
      "Epoch:    2/5     Loss: 4.203418606042862\n",
      "\n",
      "Epoch:    2/5     Loss: 4.281730809211731\n",
      "\n",
      "Epoch:    2/5     Loss: 4.144045569419861\n",
      "\n",
      "Epoch:    2/5     Loss: 3.9333086996078492\n",
      "\n",
      "Epoch:    2/5     Loss: 4.040463302612305\n",
      "\n",
      "Epoch:    3/5     Loss: 3.939927311098594\n",
      "\n",
      "Epoch:    3/5     Loss: 3.8128743925094604\n",
      "\n",
      "Epoch:    3/5     Loss: 3.9036639001369475\n",
      "\n",
      "Epoch:    3/5     Loss: 3.82889416384697\n",
      "\n",
      "Epoch:    3/5     Loss: 3.6628991487026217\n",
      "\n",
      "Epoch:    3/5     Loss: 3.772681785583496\n",
      "\n",
      "Epoch:    4/5     Loss: 3.6849228270227017\n",
      "\n",
      "Epoch:    4/5     Loss: 3.585073459148407\n",
      "\n",
      "Epoch:    4/5     Loss: 3.64347318649292\n",
      "\n",
      "Epoch:    4/5     Loss: 3.590347488641739\n",
      "\n",
      "Epoch:    4/5     Loss: 3.449392304420471\n",
      "\n",
      "Epoch:    4/5     Loss: 3.563314992189407\n",
      "\n",
      "Epoch:    5/5     Loss: 3.5091013300533884\n",
      "\n",
      "Epoch:    5/5     Loss: 3.4228617663383485\n",
      "\n",
      "Epoch:    5/5     Loss: 3.471121150970459\n",
      "\n",
      "Epoch:    5/5     Loss: 3.427291077852249\n",
      "\n",
      "Epoch:    5/5     Loss: 3.3140580189228057\n",
      "\n",
      "Epoch:    5/5     Loss: 3.4233223176002503\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/torch/serialization.py:193: UserWarning: Couldn't retrieve source code for container of type RNN. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Trained and Saved\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(vocab_size, output_size, embedding_dim, hidden_dim, n_layers, dropout=0.5)\n",
    "if train_on_gpu:\n",
    "    rnn.cuda()\n",
    "\n",
    "optimizer = torch.optim.Adam(rnn.parameters(), lr=learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "trained_rnn = train_rnn(rnn, batch_size, optimizer, criterion, num_epochs, show_every_n_batches)\n",
    "\n",
    "helper.save_model('./save/trained_rnn', trained_rnn)\n",
    "print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import helper\n",
    "import problem_unittests as tests\n",
    "\n",
    "_, vocab_to_int, int_to_vocab, token_dict = helper.load_preprocess()\n",
    "trained_rnn = helper.load_model('./save/trained_rnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def generate(rnn, prime_id, int_to_vocab, token_dict, pad_value, predict_len=100):\n",
    "    rnn.eval()\n",
    "    \n",
    "    current_seq = np.full((1, sequence_length), pad_value)\n",
    "    current_seq[-1][-1] = prime_id\n",
    "    predicted = [int_to_vocab[prime_id]]\n",
    "    \n",
    "    for _ in range(predict_len):\n",
    "        if train_on_gpu:\n",
    "            current_seq = torch.LongTensor(current_seq).cuda()\n",
    "        else:\n",
    "            current_seq = torch.LongTensor(current_seq)\n",
    "        \n",
    "        hidden = rnn.init_hidden(current_seq.size(0))\n",
    "        \n",
    "        output, _ = rnn(current_seq, hidden)\n",
    "        \n",
    "        p = F.softmax(output, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() \n",
    "         \n",
    "        top_k = 5\n",
    "        p, top_i = p.topk(top_k)\n",
    "        top_i = top_i.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        word_i = np.random.choice(top_i, p=p/p.sum())\n",
    "        \n",
    "        word = int_to_vocab[word_i]\n",
    "        predicted.append(word)     \n",
    "        \n",
    "        current_seq = np.roll(current_seq, -1, 1)\n",
    "        current_seq[-1][-1] = word_i\n",
    "    \n",
    "    gen_sentences = ' '.join(predicted)\n",
    "    \n",
    "    for key, token in token_dict.items():\n",
    "        ending = ' ' if key in ['\\n', '(', '\"'] else ''\n",
    "        gen_sentences = gen_sentences.replace(' ' + token.lower(), key)\n",
    "    gen_sentences = gen_sentences.replace('\\n ', '\\n')\n",
    "    gen_sentences = gen_sentences.replace('( ', '(')\n",
    "    \n",
    "    return gen_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:41: UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greatly increasing memory usage. To compact weights again call flatten_parameters().\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jerry: fails frequent frequent fails fails fails fails fails fails pray.\n",
      "\n",
      "george: i think we could be prosecuted for the defendants.\n",
      "\n",
      "jerry: i know, i have to be able to get my own money.\n",
      "\n",
      "george: what are you doing?\n",
      "\n",
      "hoyt: what are you doing for?\n",
      "\n",
      "hoyt: yes. the whole, the judge is the only time that was the first, but we had to do that. you know how to be the defendants of the lipo?\n",
      "\n",
      "vandelay: i have to go to a lot of trivial of the courtroom, you know, they were going to be able to create perry of these manner, i got it in the house, and we were in the plane.\n",
      "\n",
      "george: no.\n",
      "\n",
      "[new witness: the robbery]\n",
      "\n",
      "jerry: what?\n",
      "\n",
      "george: yes, i think you were.\n",
      "\n",
      "elaine: yes, i think we could have to be able to create it.\n",
      "\n",
      "elaine: you don't have to get to be the contest.\n",
      "\n",
      "jerry: oh yeah..\n",
      "\n",
      "[new nazi: yev donald.\n",
      "\n",
      "hoyt: i know.\n",
      "\n",
      "chiles: what?\n",
      "\n",
      "hoyt: i think it was in mortal uniforms.\n",
      "\n",
      "elaine: oh, no.\n",
      "\n",
      "hoyt: so, i was thinking about that term.\n",
      "\n",
      "vandelay: you know, i don't have to tell him about a lot of justice, and we have to be a lot of people, and i would wear it.\n",
      "\n",
      "elaine: oh, no. no. no. i don't care.\n",
      "\n",
      "[new nazi: what do they have to say that?\n",
      "\n",
      "hoyt: yes.\n",
      "\n",
      "george: yeah, i have a little.\n",
      "\n",
      "elaine: what?\n",
      "\n",
      "elaine: i think it was the one.\n",
      "\n",
      "hoyt: yes, i think it's a good spot.\n",
      "\n",
      "jerry: what are you talking about?\n",
      "\n",
      "hoyt: the state of the robbery].\n",
      "\n",
      "hoyt: you know, i was thinking about that witness. i know, i have a good time in the contest.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "gen_length = 400 \n",
    "prime_word = 'jerry' \n",
    "pad_word = helper.SPECIAL_WORDS['PADDING']\n",
    "generated_script = generate(trained_rnn, vocab_to_int[prime_word + ':'], int_to_vocab, token_dict, vocab_to_int[pad_word], gen_length)\n",
    "print(generated_script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "f =  open(\"generated_script_1.txt\",\"w\")\n",
    "f.write(generated_script)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal\n",
    " \n",
    "from contextlib import contextmanager\n",
    " \n",
    "import requests\n",
    " \n",
    " \n",
    "DELAY = INTERVAL = 4 * 60  # interval time in seconds\n",
    "MIN_DELAY = MIN_INTERVAL = 2 * 60\n",
    "KEEPALIVE_URL = \"https://nebula.udacity.com/api/v1/remote/keep-alive\"\n",
    "TOKEN_URL = \"http://metadata.google.internal/computeMetadata/v1/instance/attributes/keep_alive_token\"\n",
    "TOKEN_HEADERS = {\"Metadata-Flavor\":\"Google\"}\n",
    " \n",
    " \n",
    "def _request_handler(headers):\n",
    "    def _handler(signum, frame):\n",
    "        requests.request(\"POST\", KEEPALIVE_URL, headers=headers)\n",
    "    return _handler\n",
    " \n",
    " \n",
    "@contextmanager\n",
    "def active_session(delay=DELAY, interval=INTERVAL):\n",
    "    \"\"\"\n",
    "    Example:\n",
    " \n",
    "    from workspace_utils import active_session\n",
    " \n",
    "    with active_session():\n",
    "        # do long-running work here\n",
    "    \"\"\"\n",
    "    token = requests.request(\"GET\", TOKEN_URL, headers=TOKEN_HEADERS).text\n",
    "    headers = {'Authorization': \"STAR \" + token}\n",
    "    delay = max(delay, MIN_DELAY)\n",
    "    interval = max(interval, MIN_INTERVAL)\n",
    "    original_handler = signal.getsignal(signal.SIGALRM)\n",
    "    try:\n",
    "        signal.signal(signal.SIGALRM, _request_handler(headers))\n",
    "        signal.setitimer(signal.ITIMER_REAL, delay, interval)\n",
    "        yield\n",
    "    finally:\n",
    "        signal.signal(signal.SIGALRM, original_handler)\n",
    "        signal.setitimer(signal.ITIMER_REAL, 0)\n",
    " \n",
    " \n",
    "def keep_awake(iterable, delay=DELAY, interval=INTERVAL):\n",
    "    \"\"\"\n",
    "    Example:\n",
    " \n",
    "    from workspace_utils import keep_awake\n",
    " \n",
    "    for i in keep_awake(range(5)):\n",
    "        # do iteration with lots of work here\n",
    "    \"\"\"\n",
    "    with active_session(delay, interval): yield from iterable"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
